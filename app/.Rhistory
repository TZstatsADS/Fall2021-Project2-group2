# check and install needed packages
packages.used = c('tidytext', 'tidyverse', 'DT','wordcloud', 'wordcloud2',
'htmlwidgets', 'plotly', 'RColorBrewer', 'sentimentr', 'stringr', 'tm')
packages.needed = setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# check and install needed packages
packages.used = c('tidytext', 'tidyverse', 'DT','wordcloud', 'wordcloud2',
'htmlwidgets', 'plotly', 'RColorBrewer', 'sentimentr', 'stringr', 'tm')
packages.needed = setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(stringr)
library(tidytext)
install.packages(packages.needed, dependencies = TRUE)
install.packages("tidytext")
install.packages("tidytext")
install.packages("tidytext")
install.packages("tidytext")
library("tidyverse", lib.loc="~/anaconda3/envs/applied_ds_r/lib/R/library")
library("tidyselect", lib.loc="~/anaconda3/envs/applied_ds_r/lib/R/library")
install.packages("tidytext")
install.packages("tidytext")
install.packages("tidytext")
install.packages("tidytext")
# check and install needed packages
packages.used = c('tidytext', 'tidyverse', 'DT','wordcloud', 'wordcloud2',
'htmlwidgets', 'plotly', 'RColorBrewer', 'sentimentr', 'stringr', 'tm')
packages.needed = setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(stringr)
library(tidytext)
library(tidyverse)
library(DT)
library(htmlwidgets)
library(plotly)
library(RColorBrewer)
library(sentimentr)
library(wordcloud)
library(wordcloud2)
library(tm)
source('../lib/functions.R')
# check and install needed packages
packages.used = c('tidytext', 'tidyverse', 'DT','wordcloud', 'wordcloud2',
'htmlwidgets', 'plotly', 'RColorBrewer', 'sentimentr', 'stringr', 'tm')
packages.needed = setdiff(packages.used,
intersect(installed.packages()[,1],
packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(stringr)
library(tidytext)
library(tidyverse)
library(DT)
library(htmlwidgets)
library(plotly)
library(RColorBrewer)
library(sentimentr)
library(wordcloud)
library(wordcloud2)
library(tm)
source('../lib/functions.R')
print(R.version)
# read csv
data <- read_csv("../data/philosophy_data.csv")
head(data)
# add number of tokens
new_data <- data %>%
mutate(n_tokens = f.word_count(data$tokenized_txt))
# into a format that can be used to plot
tidy_df <- data %>%
dplyr::select(school, author, title) %>%
group_by(school, author) %>%
summarize(n_title = n_distinct(title),
n_sent = n()) %>%
pivot_longer(cols = c("n_title", "n_sent"),
names_to = "type",
values_to = "count")
# data frame that shows school, author, title and the nu,ber of sentences in each title
title.per.school <- data %>%
dplyr::select(school, author,title) %>%
group_by(school, author, title)%>%
summarise(n_sent = n())
title.per.author <- data %>%
dplyr::select(author, title) %>%
group_by(author, title) %>%
summarise(n_sent = n())
datatable(title.per.school, options = list(pageLength=5))
datatable(title.per.author, options = list(pageLength=5))
# assign each school with a different color with consistency
title.per.school$school <- factor(title.per.school$school, levels = c(unique(title.per.school$school)))
schools <- c(levels(title.per.school$school))
getPalette <- colorRampPalette(brewer.pal(12, 'Set3'))
colors <- getPalette(13)
names(colors)<- schools
# plot for number of titles per philosophical school
g1 <- ggplot(title.per.school)+
geom_bar(aes(x= fct_infreq(school), fill=school))+
scale_color_manual(values= colors)+
labs(title = 'Number of titles per philosophical school',
x= 'School',
y= 'Titles (count)')+
theme(legend.position = "none")+
coord_flip()
g1
g2 <- tidy_df %>% filter(type== "n_sent") %>%
ggplot(aes(log10(count)))+
geom_histogram(binwidth = .1, color= "black")+
geom_density()+
labs(title = "Number of sentences for each title",
x= "log10(number of sentences)",
y="Frequency")
g2
g3 <- data %>%
dplyr::select(school) %>%
ggplot()+
geom_histogram(aes(fct_infreq(school), fill = school), stat="count")+
scale_color_manual(values= colors)+
labs(title= "Number of sentences per philosophical school",
x= "School",
y= "Sentences (count)")+
coord_flip()+
theme(legend.position = "none")
g3
par(mfrow=c(2,1))
g1; g3
g4 <- tidy_df %>%
group_by(school, type) %>%
summarise(count= sum(count)) %>%
ggplot(aes(log10(count), fct_reorder2(school, type=="n_sent", log10(count), .desc = FALSE), color=type))+
geom_point()+
labs(title= "Comparison of number of titles and sentences per school",
x= "log10(count)",
y= "School")+
scale_color_manual(name="Type",
labels=c("# of sentences","# of titles"),
values=c("red","blue"))
g4
g5 <- data %>%
dplyr::select(school, author, title) %>%
ggplot()+
geom_bar(aes(fct_infreq(title), fill=school))+
scale_x_discrete(label = function(x) abbreviate(x, minlength = 7))+
theme(axis.text.x = element_text(angle=45, hjust = 1, vjust=0.5))+
labs(title = "Number of sentences per title",
x= "Title (abbreviated)",
y= "Number of sentences")
ggplotly(g5)
mult.authors <- tidy_df %>%
filter(type == "n_title") %>%
filter(count > 1) %>%
ungroup() %>%
dplyr::select(author) %>% unlist()
g6 <- data %>%
dplyr::select(school, author, title) %>%
filter(author %in% mult.authors) %>%
ggplot()+
geom_bar(aes(fct_infreq(title), fill=author))+
scale_x_discrete(label = function(x) abbreviate(x, minlength = 7))+
theme(axis.text.x = element_text(angle=45, hjust = 1, vjust=0.5))+
geom_hline(yintercept = mean(title.per.author$n_sent), size=1, col='gray')
ggplotly(g6)
sent.len.df <- new_data %>%
dplyr::select(school, author, title, sentence_length, n_tokens) %>%
group_by(school, author, title) %>%
summarize(sent_len = sum(sentence_length), n_tokens = sum(n_tokens))
datatable(sent.len.df, options = list(pageLength=5))
len.avg.df <- new_data %>%
dplyr::select(school, author, title, sentence_length, n_tokens) %>%
group_by(school, author, title) %>%
summarize(sent_len = mean(sentence_length), n_tokens = mean(n_tokens))
# number of words in a sentence
datatable(len.avg.df, options = list(pageLength=5))
length_info <-summary(data$sentence_length)
length_info
g.len <- new_data %>%
dplyr::select(sentence_length) %>%
ggplot(aes(log10(sentence_length)))+
geom_histogram(bins = 50, color="black")+
labs(title= "Sentence length distribution",
x= "log10(Sentence length)",
y= "Frequency")
token_info <- summary(new_data$n_tokens)
token_info
g.token <- new_data %>%
dplyr::select(n_tokens) %>%
ggplot(aes(log10(n_tokens)))+
geom_histogram(bins = 50, color= "black")+
labs(title= "Number of tokens distribution",
x= "log10(Number of tokens)",
y= "Frequency")
par(mfrow= c(1, 2))
g.len ; g.token
id <- which(new_data$n_tokens == 0)
new_data[id,c("sentence_str", "tokenized_txt")]
#filter out for rows that holds sentences without meaning
new_data <- new_data %>%
filter(n_tokens >0)
summary(new_data$n_tokens)
corr_df <- title.per.school %>%
left_join(sent.len.df, by = c("school", "author", "title")) %>%
ungroup() %>%
dplyr::select(title, n_sent, n_tokens)
g.cor <- ggplot(corr_df)+
geom_point(aes(log10(n_sent), log10(n_tokens)))
g.cor
shapiro.test(corr_df$n_sent)
shapiro.test(corr_df$n_tokens)
result <- cor.test(corr_df$n_sent, corr_df$n_tokens, method = "kendall")
result
mult.authors
mlt_pub <- data %>%
dplyr::select(author, title, original_publication_date, tokenized_txt) %>%
filter(author %in% mult.authors)
# create dataframe for each author
niet <- mlt_pub %>% filter(author == "Nietzsche")
hegel <- mlt_pub %>% filter(author == "Hegel")
kant <- mlt_pub %>% filter(author == "Kant")
niet %>% select(title, original_publication_date) %>%
distinct(title, .keep_all=TRUE) %>% arrange(original_publication_date)
niet_txt1 <- niet %>% filter(original_publication_date==1886)
niet_txt2 <- niet %>% filter(original_publication_date==1887)
niet_txt3 <- niet %>% filter(original_publication_date==1888)
par(mfrow= c(1, 3))
create_wc(niet_txt1); create_wc(niet_txt2); create_wc(niet_txt3)
hegel %>% select(title, original_publication_date) %>%
distinct(title, .keep_all=TRUE) %>% arrange(original_publication_date)
hegel_txt1 <- hegel %>% filter(original_publication_date==1807)
hegel_txt2 <- hegel %>% filter(original_publication_date==1817)
hegel_txt3 <- hegel %>% filter(original_publication_date==1820)
par(mfrow= c(1, 3))
create_wc(hegel_txt1); create_wc(hegel_txt2); create_wc(hegel_txt3)
# Check and install needed packages. Load the libraries.
packages.used=c("tibble", "tidyverse","dplyr", "wordcloud", "RColorBrewer", "tm", "tidytext")
# check packages that need to be installed.
packages.needed=setdiff(packages.used,intersect(installed.packages()[,1],packages.used))
# install additional packages
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
# load packages
library(tibble)
library(tidyverse)
library(dplyr)
library(wordcloud)
library(RColorBrewer)
library(tm)
library(tidytext)
print(R.version)
# Load and explore the dataset to get an overview.
folderpath<-"../data/philosophy_data.csv"
df_org<-read.csv(folderpath)
# Load and explore the dataset to get an overview.
folderpath<-"../data/philosophy_data.csv"
df_org<-read.csv(folderpath)
# Load and explore the dataset to get an overview.
folderpath<-"../data/philosophy_data.csv"
df_org<-read.csv(folderpath)
# Load and explore the dataset to get an overview.
folderpath<-"../data/philosophy_data.csv"
data <- read_csv("../data/philosophy_data.csv")
# Load and explore the dataset to get an overview.
folderpath<-"../data/philosophy_data.csv"
data <- read_csv("../data/philosophy_data.csv")
head(df_org)
# Load and explore the dataset to get an overview.
folderpath<-"../data/philosophy_data.csv"
df_org <- read_csv("../data/philosophy_data.csv")
head(df_org)
#pick out the sentences that contain the key words about the topic: love
df<-df_org%>%select(school,original_publication_date,sentence_lowered)%>%filter(grepl('love',sentence_lowered))
#divide the development of philosophy by period
df<-df%>%mutate(period=case_when(original_publication_date < 1000~ 'ancient',
1000<=original_publication_date & original_publication_date<1500~'medieval',
1500<=original_publication_date & original_publication_date<1900~'modern',
original_publication_date>=1900~'contemporary'))
#check is there's any NAs in the data
sum(is.na(df))
#save the final dataset used for subsetting is saved in output file
write_csv(df,"../output/processed_philosopy.csv")
#subset dataframes by different periods
ancient<-df%>%filter(period=='ancient')
modern<-df%>%filter(period=='modern')
contemporary<-df%>%filter(period=='contemporary')
#get the unique value of school
school<-as_tibble(unique(df$school))
print(school)
#subset dataframes by different schools
plato<-df%>%filter(school=='plato')
aristotle<-df%>%filter(school=='aristotle')
rationalism<-df%>%filter(school=='rationalism')
nietzsche<-df%>%filter(school=='nietzsche')
feminism<-df%>%filter(school=='feminism')
#define the stop words
stopword<-as.character(stop_words$word)
#define a function for text mining and visualization
clean<-function(sub){
sub<-as.data.frame(sub)
title=paste0(unique(sub$school),' Word Frequency Chart')
corpus<-VCorpus(VectorSource(sub$sentence_lowered))%>%
tm_map(removeWords, stopword)%>%
tm_map(removePunctuation)%>%
tm_map(removeNumbers)%>%
tm_map(removeWords, character(0))%>%
tm_map(stripWhitespace)
sub.tidy=tidy(TermDocumentMatrix(corpus))
overall=summarise(group_by(sub.tidy, term), sum(count))
overall<-overall%>%filter(!grepl('love',term))%>%rename('n'='sum(count)')
#obtain top 20 most frequent words about love
temp<-overall%>%arrange(desc(n))
p1<-temp[1:20,]%>%mutate(term=reorder(term,n))%>%
ggplot(aes(term, n)) +
geom_col() +
xlab(NULL) +
ylab("Word Frequency")+
labs(title=title)+
coord_flip()
print(p1)
#obtain word cloud
wordcloud(overall$term, overall$n,
scale=c(5,0.5),
max.words=100,
min.freq=1,
random.order=FALSE,
rot.per=0.3,
use.r.layout=T,
random.color=FALSE,
colors=brewer.pal(9,"Blues"))
}
clean(ancient)
clean(modern)
clean(contemporary)
clean(aristotle)
clean(plato)
clean(rationalism)
clean(nietzsche)
clean(feminism)
shiny::runApp('Desktop/Columbia Masters/Applied Data Science/Fall2021-Project2-group-2/app')
install.packages("highcharter")
install.packages("install.packages("highcharter")")
install.packages("highcharter")
install.packages("highcharter")
shiny::runApp('Desktop/Columbia Masters/Applied Data Science/Fall2021-Project2-group-2/app')
NYC_Dog_Licensing_Dataset <- read_csv("NYC_Dog_Licensing_Dataset.csv",
col_types = cols( LicenseIssuedDate = col_date(format = "%m/%d/%Y")))
setwd("~/Desktop/Columbia Masters/Applied Data Science/Fall2021-Project2-group-2/app")
NYC_Dog_Licensing_Dataset <- read_csv("NYC_Dog_Licensing_Dataset.csv",
col_types = cols( LicenseIssuedDate = col_date(format = "%m/%d/%Y")))
View(NYC_Dog_Licensing_Dataset)
View(NYC_Dog_Licensing_Dataset)
valueBox(
value =  paste0("+",
round(((sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2020-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2020-12-31",]$freq)-
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq))/
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq)*100),
digits=2),"%"
),
subtitle = tags$p("2020 YoY New Applications 07/20-12/31", style = "color:black"),
icon = icon('export', lib = 'glyphicon'),
color = "green"
)
NYC_Dog_Licensing_Dataset <- count(NYC_Dog_Licensing_Dataset,
"LicenseIssuedDate")
View(NYC_Dog_Licensing_Dataset)
View(NYC_Dog_Licensing_Dataset)
valueBox(
value =  paste0("+",
round(((sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2020-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2020-12-31",]$freq)-
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq))/
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq)*100),
digits=2),"%"
),
subtitle = tags$p("2020 YoY New Applications 07/20-12/31", style = "color:black"),
icon = icon('export', lib = 'glyphicon'),
color = "green"
)
View(NYC_Dog_Licensing_Dataset)
NYC_Dog_Licensing_Dataset$freq <- count(NYC_Dog_Licensing_Dataset,
"LicenseIssuedDate")
View(NYC_Dog_Licensing_Dataset)
NYC_Dog_Licensing_Dataset <- read_csv("NYC_Dog_Licensing_Dataset.csv",
col_types = cols( LicenseIssuedDate = col_date(format = "%m/%d/%Y")))
NYC_Dog_Licensing_Dataset$freq <- count(NYC_Dog_Licensing_Dataset,
"LicenseIssuedDate")
valueBox(
value =  paste0("+",
round(((sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2020-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2020-12-31",]$freq)-
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq))/
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq)*100),
digits=2),"%"
),
subtitle = tags$p("2020 YoY New Applications 07/20-12/31", style = "color:black"),
icon = icon('export', lib = 'glyphicon'),
color = "green"
)
NYC_Dog_Licensing_Dataset <- read_csv("../data/NYC_Dog_Licensing_Dataset.csv",
col_types = cols(RowNumber = col_skip(),
AnimalName = col_skip(), AnimalGender = col_skip(),
AnimalBirthMonth = col_skip(), BreedName = col_skip(),
Borough = col_skip(),
LicenseIssuedDate = col_date(format = "%m/%d/%Y"),
LicenseExpiredDate = col_skip(),
`Extract Year` = col_skip(),
`Unique Dog ID` = col_skip()))
NYC_Dog_Licensing_Dataset$freq <- count(NYC_Dog_Licensing_Dataset,
"LicenseIssuedDate")
View(NYC_Dog_Licensing_Dataset)
View(NYC_Dog_Licensing_Dataset)
valueBox(
value =  paste0("+",
round(((sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2020-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2020-12-31",]$freq)-
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq))/
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq)*100),
digits=2),"%"
),
subtitle = tags$p("2020 YoY New Applications 07/20-12/31", style = "color:black"),
icon = icon('export', lib = 'glyphicon'),
color = "green"
)
View(NYC_Dog_Licensing_Dataset)
View(NYC_Dog_Licensing_Dataset)
valueBox(
value =  paste0("+",
(sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2020-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2020-12-31",]$freq)-
sum(NYC_Dog_Licensing_Dataset[NYC_Dog_Licensing_Dataset$LicenseIssuedDate >= "2019-07-20" & NYC_Dog_Licensing_Dataset$LicenseIssuedDate <= "2019-12-31",]$freq))
),
subtitle = tags$p("2020 YoY New Applications 07/20-12/31", style = "color:black"),
icon = icon('export', lib = 'glyphicon'),
color = "green"
)
runApp()
